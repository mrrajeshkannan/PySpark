{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fec91e1c-6f81-49dd-be61-9e1b4f020fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Basic To advanced PySpark query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15b36b8-cd2e-4558-b3b8-d01d67b8c318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ”¹ Cluster vs. Distributed System\n",
    "\n",
    "| Feature               | **Cluster**                                      | **Distributed System**                             |\n",
    "|----------------------|--------------------------------|---------------------------------|\n",
    "| **Definition**       | A group of interconnected computers (nodes) that work together as a single unit. | A collection of independent computers that work together but appear as a single system to users. |\n",
    "| **Architecture**     | Usually tightly coupled with shared resources (e.g., shared storage). | Loosely coupled; nodes communicate over a network. |\n",
    "| **Data Sharing**     | Nodes often have access to shared storage. | Nodes manage data separately and communicate over a network. |\n",
    "| **Fault Tolerance**  | High availability; failure of one node doesn't affect others significantly. | More fault-tolerant; designed for redundancy and recovery. |\n",
    "| **Example Technologies** | Databricks, Hadoop Cluster, Kubernetes Cluster | Apache Spark, AWS, Google Cloud, Distributed Databases |\n",
    "\n",
    "## âœ… Key Differences\n",
    "1. **A cluster is a type of distributed system**, but not all distributed systems are clusters.\n",
    "2. **Clusters are typically within a single data center**, while distributed systems **can span across multiple locations**.\n",
    "3. **Clusters often have a shared resource model**, while distributed systems focus on **scalability and fault tolerance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c321960-8408-4d51-b34d-41f8ee2acaf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DATA READING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68951798-1121-4867-9834-02102c98d7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+\n|employee_id|first_name|last_name|   email|phone_number| hire_date| job_id|salary|commission_pct|manager_id|department_id|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+\n|        100|    Steven|     King|   SKING|515.123.4567|2087-06-17|AD_PRES| 25000|          null|      null|           90|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2089-09-21|  AD_VP| 17000|          null|       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2093-01-13|  AD_VP| 17000|          null|       100|           90|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|2090-01-03|IT_PROG|  9000|          null|       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|2091-05-21|IT_PROG|  6000|          null|       103|           60|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Read Data from CSV:\n",
    "df_csv = spark.read.csv('dbfs:/FileStore/tables/employees.csv',inferSchema=True, header=True)\n",
    "#display(df_csv.limit(5))        # Alter method\n",
    "#df_csv.display()                # Alter method\n",
    "df_csv.show(5)  # Returns a list of Row objects\n",
    "\n",
    "\n",
    "\n",
    "# inferSchema means auto identify the data type in CSV / Json file.\n",
    "\n",
    "# same as json \n",
    "#df_json = spark.read.json('/FileStore/tables/employees.json',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393f036e-59d2-4ec5-9780-41c872719065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Reading Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0df3031-2af5-4d33-9b3c-09d467255a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [FileInfo(path='dbfs:/FileStore/tables/departments-1.csv', name='departments-1.csv', size=687, modificationTime=1740000633000),\n FileInfo(path='dbfs:/FileStore/tables/departments.csv', name='departments.csv', size=687, modificationTime=1738674464000),\n FileInfo(path='dbfs:/FileStore/tables/emp.json', name='emp.json', size=752, modificationTime=1739564468000),\n FileInfo(path='dbfs:/FileStore/tables/employees.csv', name='employees.csv', size=7920, modificationTime=1738674451000)]"
     ]
    }
   ],
   "source": [
    "# Csv/Json uploaded locatations in Data bricks\n",
    "dbutils.fs.ls('dbfs:/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db4ec9c-ab42-4cbb-bd6a-7fe207b60163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Delete a File in locatation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3b349a-2b7f-4256-b826-f9553f335a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Delete a Single File\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/tables/employees-1.csv\")\n",
    "\n",
    "\n",
    "#Delete All Files in the Folder\n",
    "#dbutils.fs.rm(\"dbfs:/FileStore/tables/\", recurse=True)   #The recurse=True flag ensures all files in the folder are deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c5abb4-9aed-4074-9536-5f0181a98687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema** **Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429d35fb-25ce-4bfd-a469-c258e8933cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print the description of table / Scheme\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3ae35e-4308-4940-bc08-165af901f57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Select query \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1025d3e9-0674-4cf3-a4e8-2872f6af35cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.display()   # Shows all \n",
    "\n",
    "df_csv.show(5)      # Alter method\n",
    "\n",
    "df_csv.select('first_name','salary','department_id').show(5)    # Selecting few columns we can't do column name change in this method\n",
    "\n",
    "df_csv.select(col('first_name').alias('Name'),col('salary'),col('department_id')).show(5)    # Selecting few columns with alias\n",
    "\n",
    "df_csv = spark.read.csv('/FileStore/tables/employees.csv',inferSchema=True, header=True)\n",
    "\n",
    "df_csv.describe().display()     # data frame describtion like count, mean, median, max min sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4a2a55-0b75-49a6-9116-83a136734b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Select with filter condition\n",
    "The filter transformation in PySpark allows you to select rows from a DataFrame based on a condition. \n",
    "\n",
    "### ðŸš€ Summary of `filter()` Use Cases  \n",
    "\n",
    "| Use Case               | Example  |\n",
    "|------------------------|----------|\n",
    "| **Logical Operators:(and)** | `& (AND): (condition1) & (condition2)`|\n",
    "| **Equality:**   | `col(\"column_name\") == value or col(\"column_name\") === value (The === is preferred for clarity)` |\n",
    "| **Inequality:** | `col(\"column_name\") != value or col(\"column_name\") <> value` |\n",
    "| **Greater than:** | `col(\"column_name\") > value` |\n",
    "| **Less than:** | `col(\"column_name\") < value` |\n",
    "| **Greater than or equal to:** | `col(\"column_name\") >= value` |\n",
    "| **Less than or equal to:** | `col(\"column_name\") <= value` |\n",
    "| **Handle NULL values** | `col(\"column_name\").isNull()` |\n",
    "| **Extract date parts** | `col(\"column_name\").isNotNull()` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c6bc2a-2e51-4f8c-9573-b59b25ed46fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.filter(col('department_id')==60).show(10)\n",
    "\n",
    "df_csv.filter(col('department_id').isin(60,80)).show(5)\n",
    "\n",
    "df_csv.filter(~col('department_id').isin(60,80)).show()    # selecting Notin \n",
    "\n",
    "df_csv.filter((col('department_id').isNotNull()) & (col('commission_pct').isNull())).show(5)\n",
    "\n",
    "\n",
    "\n",
    "df_csv.filter(\"department_id IS NOT NULL AND commission_pct IS NULL\").show(5)  # Alter SQL Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e68b1e-acde-4f2a-8eb8-b7aeff05dbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# withColumn\n",
    "The withColumn() function in PySpark allows you to add, update, or modify columns in a DataFrame.\n",
    "\n",
    "### ðŸš€ Summary of `withColumn()` Use Cases  \n",
    "\n",
    "| Use Case               | Example  |\n",
    "|------------------------|----------|\n",
    "| **Add a new column**   | `withColumn(\"bonus\", lit(500))` |\n",
    "| **Modify existing column** | `withColumn(\"salary\", col(\"salary\") * 1.1)` |\n",
    "| **Rename column** | `withColumn(\"emp_name\", col(\"first_name\")).drop(\"first_name\")` |\n",
    "| **Change data type** | `withColumn(\"department_id\", col(\"department_id\").cast(\"string\"))` |\n",
    "| **String transformations** | `withColumn(\"first_name_upper\", upper(col(\"first_name\")))` |\n",
    "| **Replace values** | `withColumn(\"job_id\", regexp_replace(col(\"job_id\"), \"FI_ACCOUNT\", \"ACNT\"))` |\n",
    "| **Handle NULL values** | `withColumn(\"commission_pct\", coalesce(col(\"commission_pct\"), lit(0)))` |\n",
    "| **Extract date parts** | `withColumn(\"hire_year\", year(col(\"hire_date\")))` |\n",
    "| **Conditional logic** | `withColumn(\"salary_category\", when(col(\"salary\") > 60000, \"High\").otherwise(\"Low\"))` |\n",
    "| **Mathematical operations** | `withColumn(\"yearly_salary\", col(\"salary\") * 12)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0dba79-1531-4fb9-9a60-e3490f0aed80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce, round\n",
    "\n",
    "# When you use withColumn the hole column and additional column will contain\n",
    "df_csv.withColumn(\"bonus\", lit(500)).show(5)  # Adds a new column with a fixed value\n",
    "\n",
    "df_csv.withColumn(\n",
    "    \"Full_sal\", round((col(\"salary\")) * (1 + coalesce(col(\"commission_pct\"), lit(0))), 2) # Round to 2 decimal places\n",
    ").display()\n",
    "\n",
    "\n",
    "df_new = df_csv.withColumn(\"department_id\", col(\"department_id\").cast(\"string\"))\n",
    "\n",
    "\n",
    "\n",
    "# When you use select only selected column will contain\n",
    "df_csv.select(col('first_name'), round((col(\"salary\")) * (1 + coalesce(col(\"commission_pct\"), lit(0)).alias(\"full_salary\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e0f1db-7a8f-403b-a4e6-a60424a01d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Changing the column type DDL :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ca4293-ba67-4eb4-87e0-36b92584e6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_csv = df_csv.withColumn(\"employee_id\", col(\"employee_id\").cast(\"string\"))\n",
    "\n",
    "from pyspark.sql.functions import to_date;\n",
    "df.withColumn('date', to_date(df['date_string']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b62b307c-8772-491e-972e-4b37188ad3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## StructType() Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ed188f-426c-440f-984f-52983e3e67ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**importing** **libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9d94dc-1622-4c9a-b5ca-1e9db626ed1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a50b96c-b5eb-4c3e-9e0d-8e0d042624e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sorting ascending / descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02a320e-703b-4ac7-88de-43d93fe82de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#   Method 1\n",
    "df_csv.sort(col('department_id').desc()).show(5)\n",
    "\n",
    "df_csv.orderBy(col(\"salary\").desc()).show(5)  # Alternative\n",
    "\n",
    "df_csv.sort(col('department_id').asc()).show(5) # Ascending order\n",
    "\n",
    "\n",
    "#   Method 2 with 2 or more column \n",
    "df_csv.sort(['department_id','salary'],ascending = [1,0]).show(10)     # passing 0 is false 1 is True for assending order\n",
    "\n",
    "df_csv.sort(['first_name'],ascending = [1]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1b99fe-e12b-4824-a9c2-611873bc79e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Type Casting:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc1c825-ea22-4d3e-8d65-2521c073dfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_new = df_csv.withColumn('department_id',col('department_id').cast(StringType()))\n",
    "#df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b063bf2-2eba-4d54-927c-3c9d9d72f191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95dcef9-858a-4bfe-b082-0e6d9a08e901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop / remove column:-\n",
    "df_csv.drop('phone_number','email').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c752536-de0c-4074-9252-9571f6b82559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Drop_Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53125f2f-abbc-491e-a73a-69e51dec34f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicates with all columns \n",
    "df_csv.distinct().display()\n",
    "\n",
    "\n",
    "# remove duplicates with all column and specfic column also \n",
    "df_csv.dropDuplicates().display()\n",
    "\n",
    "df_dis = df_csv.dropDuplicates(subset=['employee_id'])      # when you go with column duplicate use this method\n",
    "df_dis.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a03a67-53ee-4d28-b0a0-3672da37d763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Count number of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c5ac47-d63f-40f8-8fb5-b25b7f25f909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 1: Using count() action\n",
    "row_count = df_csv.count()\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "\n",
    "# Method 2: Using describe() to get summary statistics (includes count)\n",
    "summary = df_csv.describe()\n",
    "row_count_from_summary = int(summary.filter(\"summary == 'count'\").collect()[0][1]) #extracting the count value\n",
    "print(f\"Number of rows (from describe): {row_count_from_summary}\")\n",
    "\n",
    "# Method 3: Using length (if you've already collected the data - less efficient for large datasets)\n",
    "# Avoid this for large DataFrames as it brings all data to the driver\n",
    "# df_collected = df_csv.collect()  # Only use if DataFrame is small\n",
    "# row_count_collected = len(df_collected)\n",
    "# print(f\"Number of rows (collected): {row_count_collected}\")\n",
    "\n",
    "# Method 4: Using rdd.count()\n",
    "row_count_rdd = df_csv.rdd.count()\n",
    "print(f\"Number of rows (rdd): {row_count_rdd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9d22fb-caef-457a-b4c0-221a112785af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Union & UnionByName\n",
    "\n",
    "### ðŸ” Key Differences in PySpark Union Functions\n",
    "\n",
    "| Method | Handles Column Order? | Handles Extra Columns? | Removes Duplicates? |\n",
    "|--------|------------------|------------------|------------------|\n",
    "| `union()` | âŒ No | âŒ No | âŒ No |\n",
    "| `unionAll()` | âŒ No | âŒ No | âŒ No |\n",
    "| `union().distinct()` | âŒ No | âŒ No | âœ… Yes |\n",
    "| `unionByName()` | âœ… Yes | âŒ No | âŒ No |\n",
    "| `unionByName(allowMissingColumns=True)` | âœ… Yes | âœ… Yes | âŒ No |\n",
    "\n",
    "---\n",
    "### **ðŸ’¡ Best Practices**\n",
    "- Use **`unionByName()`** when working with DataFrames that have **different column orders**.\n",
    "- Use **`allowMissingColumns=True`** when columns are missing between DataFrames.\n",
    "- Use **`.distinct()`** to **remove duplicates** after a union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8049c2dd-7170-480e-9003-278ce6322ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparatation : \n",
    "data1 = [('kad','1'),\n",
    "           ('sid','2')]\n",
    "schema1 = 'name STRING, id STRING' \n",
    "\n",
    "df1 = spark.createDataFrame(data1,schema1)        # creating data frame 1\n",
    "\n",
    "\n",
    "data2 = [('3','rahul'),\n",
    "        ('4','jas')]\n",
    "schema2 = 'id STRING, name STRING' \n",
    "\n",
    "df2 = spark.createDataFrame(data2,schema2)      # creating data frame 2\n",
    "\n",
    "#df1.display()   \n",
    "df1.display()\n",
    "df2.display()\n",
    "\n",
    "df1.union(df2).show()\n",
    "\n",
    "df1.unionByName(df2).show()         # See the difference in result \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48632257-ad9a-449c-a8bf-565d7abcbacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Œ String Functions\n",
    "\n",
    "PySpark provides various string functions to manipulate and process string data. Below is a categorized list of commonly used **string functions**:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Basic String Operations\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `upper()` | Converts to uppercase | `df.select(upper(col(\"name\"))).show()` |\n",
    "| `lower()` | Converts to lowercase | `df.select(lower(col(\"name\"))).show()` |\n",
    "| `length()` | Returns string length | `df.select(length(col(\"name\"))).show()` |\n",
    "| `trim()` | Removes spaces from both sides | `df.select(trim(col(\"name\"))).show()` |\n",
    "| `ltrim()` | Removes spaces from the left | `df.select(ltrim(col(\"name\"))).show()` |\n",
    "| `rtrim()` | Removes spaces from the right | `df.select(rtrim(col(\"name\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Substring & Character Extraction\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `substring(col, start, length)` | Extracts substring from a column | `df.select(substring(col(\"name\"), 1, 3)).show()` |\n",
    "| `split(col, pattern)` | Splits string into an array | `df.select(split(col(\"name\"), \" \")).show()` |\n",
    "| `instr(col, substr)` | Finds position of substring | `df.select(instr(col(\"name\"), \"a\")).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. String Replacement & Formatting\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `regexp_replace(col, pattern, replacement)` | Replaces substrings using regex | `df.select(regexp_replace(col(\"name\"), \"John\", \"Mike\")).show()` |\n",
    "| `translate(col, fromStr, toStr)` | Replaces characters | `df.select(translate(col(\"name\"), \"abc\", \"xyz\")).show()` |\n",
    "| `format_string(format, col1, col2, ...)` | Formats string | `df.select(format_string(\"%s - %s\", col(\"name\"), col(\"job\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. String Case & Padding\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `initcap(col)` | Capitalizes first letter of each word | `df.select(initcap(col(\"name\"))).show()` |\n",
    "| `lpad(col, length, pad)` | Left-pads string with characters | `df.select(lpad(col(\"name\"), 10, \"0\")).show()` |\n",
    "| `rpad(col, length, pad)` | Right-pads string with characters | `df.select(rpad(col(\"name\"), 10, \"0\")).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. String Comparison & Matching\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `like(col, pattern)` | SQL-like pattern matching (`%`, `_`) | `df.filter(col(\"name\").like(\"A%\")).show()` |\n",
    "| `rlike(col, regex)` | Regex pattern matching | `df.filter(col(\"name\").rlike(\"^A.*\")).show()` |\n",
    "| `contains(col, substr)` | Checks if string contains substring | `df.filter(col(\"name\").contains(\"John\")).show()` |\n",
    "| `startsWith(col, prefix)` | Checks if string starts with substring | `df.filter(col(\"name\").startswith(\"A\")).show()` |\n",
    "| `endsWith(col, suffix)` | Checks if string ends with substring | `df.filter(col(\"name\").endswith(\"z\")).show()` |\n",
    "\n",
    "\n",
    "##  ðŸ”¹ 6. Concat two or more columns\n",
    "\n",
    "| Method | Use Case | Example |\n",
    "|--------|---------|---------|\n",
    "| `concat(col1, col2)` | Joins multiple string columns | `concat(col(\"first_name\"), col(\"last_name\"))` |\n",
    "| `concat_ws(\" \", col1, col2)` | Joins columns with a separator | `concat_ws(\"-\", col(\"city\"), col(\"state\"))` |\n",
    "| `concat(col1, lit(\" \"), col2)` | Adds static text between columns | `concat(col(\"first_name\"), lit(\" - \"), col(\"last_name\"))` |\n",
    "| `concat(col1, col2.cast(\"string\"))` | Converts non-string columns before joining | `concat(col(\"first_name\"), col(\"salary\").cast(\"string\"))` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e22375-b0b6-4442-af91-7a1733562b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, trim, ltrim, rtrim, substring, split, instr, regexp_replace, translate, format_string, initcap, lpad, rpad\n",
    "\n",
    "from pyspark.sql.functions import *     # you can use this \n",
    "\n",
    "df_csv.select(col('first_name'), upper(col(\"first_name\")), \n",
    "              lower(col(\"first_name\")).alias('Name'), length(col(\"first_name\")),\n",
    "              concat(col('first_name'), col('last_name')).alias(\"full_name\") ,\n",
    "              concat(col('first_name'), lit(' '), col('last_name')).alias(\"full_name\") \n",
    "            ).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c49872f5-b03a-436b-a007-5e71b8930f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“† Date Functions  \n",
    "\n",
    "PySpark provides various **date and time functions** for handling and manipulating date-related data. Below is a categorized list of commonly used **date functions** in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **1. Extracting Date Components**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `year(col(\"date\"))` | Extracts the year from a date | `df.select(year(col(\"hire_date\"))).show()` |\n",
    "| `month(col(\"date\"))` | Extracts the month from a date | `df.select(month(col(\"hire_date\"))).show()` |\n",
    "| `day(col(\"date\"))` | Extracts the day of the month | `df.select(dayofmonth(col(\"hire_date\"))).show()` |\n",
    "| `dayofweek(col(\"date\"))` | Extracts the day of the week (1 = Sunday, 7 = Saturday) | `df.select(dayofweek(col(\"hire_date\"))).show()` |\n",
    "| `dayofyear(col(\"date\"))` | Extracts the day of the year | `df.select(dayofyear(col(\"hire_date\"))).show()` |\n",
    "| `quarter(col(\"date\"))` | Extracts the quarter of the year | `df.select(quarter(col(\"hire_date\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **2. Date Arithmetic & Manipulation**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `date_add(col(\"date\"), n)` | Adds `n` days to a date | `df.select(date_add(col(\"hire_date\"), 10)).show()` |\n",
    "| `date_sub(col(\"date\"), n)` | Subtracts `n` days from a date | `df.select(date_sub(col(\"hire_date\"), 10)).show()` |\n",
    "| `add_months(col(\"date\"), n)` | Adds `n` months to a date | `df.select(add_months(col(\"hire_date\"), 3)).show()` |\n",
    "| `months_between(col(\"date1\"), col(\"date2\"))` | Returns months between two dates | `df.select(months_between(col(\"hire_date\"), col(\"current_date\"))).show()` |\n",
    "| `next_day(col(\"date\"), \"Day\")` | Returns the next occurrence of the specified day | `df.select(next_day(col(\"hire_date\"), \"Friday\")).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **3. Converting & Formatting Dates**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `to_date(col(\"string_col\"), \"format\")` | Converts string to date | `df.select(to_date(col(\"date_str\"), \"yyyy-MM-dd\")).show()` |\n",
    "| `date_format(col(\"date\"), \"format\")` | Formats a date column | `df.select(date_format(col(\"hire_date\"), \"yyyy/MM/dd\")).show()` |\n",
    "| `unix_timestamp(col(\"date\"))` | Converts date to Unix timestamp | `df.select(unix_timestamp(col(\"hire_date\"))).show()` |\n",
    "| `from_unixtime(col(\"unix_col\"))` | Converts Unix timestamp to date | `df.select(from_unixtime(col(\"unix_col\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **4. Time Functions**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `current_date()` | Returns the current system date | `df.select(current_date()).show()` |\n",
    "| `current_timestamp()` | Returns the current timestamp | `df.select(current_timestamp()).show()` |\n",
    "| `datediff(col(\"date1\"), col(\"date2\"))` | Returns days between two dates | `df.select(datediff(col(\"hire_date\"), current_date())).show()` |\n",
    "| `timestamp_seconds(col(\"unix_col\"))` | Converts Unix timestamp (seconds) to timestamp | `df.select(timestamp_seconds(col(\"unix_col\"))).show()` |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ca5085-f620-43ad-aeee-631fae52cb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col, year, month, dayofmonth, date_add, to_date, current_date, datediff\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "df_csv.select(\n",
    "                col('hire_date'),\n",
    "                year(col('hire_date')).alias('year'),\n",
    "                month(col('hire_date')).alias('Mon'),\n",
    "                dayofmonth(col('hire_date')).alias('day'),\n",
    "                dayofweek('hire_date').alias('week_of_mon'),\n",
    "                add_months(col('hire_date'),3).alias('add_mon'),\n",
    "                add_months(col('hire_date'),-3).alias('sub_mon'),\n",
    "                date_add(col('hire_date'),5).alias('add_date'),\n",
    "                date_sub(col('hire_date'),5).alias('add_date'),\n",
    "                date_trunc('Month',col('hire_date')).alias('first_day_of_mon'),\n",
    "                datediff(current_date(),col('hire_date')).alias('date_diff'),\n",
    "                current_date()\n",
    "            ).show(5)\n",
    "\n",
    "\n",
    "# here showing difference between withColumn and select \n",
    "df_csv.withColumn('current_Date', current_date()).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9569b96c-fc16-4bb2-9ad5-3a652c38ee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸš€ Handling NULLs \n",
    "\n",
    "Handling **NULL** values is crucial when working with large datasets in PySpark. Below is a summary of common techniques to manage **NULL** values effectively.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **1. Filtering NULL Values**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Filter rows where column is NOT NULL | `df.filter(col(\"salary\").isNotNull()).show()` |\n",
    "| Filter rows where column IS NULL | `df.filter(col(\"salary\").isNull()).show()` |\n",
    "| Drop rows containing NULLs in any column | `df.na.drop().show()` |\n",
    "| Drop rows only if specific column(s) are NULL | `df.na.drop(subset=[\"salary\"]).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **2. Replacing NULL Values**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Replace NULLs with a default value | `df.na.fill(0).show()` |\n",
    "| Replace NULLs in specific column | `df.na.fill({\"salary\": 50000, \"department\": \"Unknown\"}).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **3. Using `coalesce()` for Default Values**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Replace NULLs with another column value | `df.withColumn(\"final_salary\", coalesce(col(\"salary\"), lit(50000))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ **4. Handling NULLs in Aggregations**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Ignore NULLs in aggregations (default behavior) | `df.select(avg(\"salary\")).show()` |\n",
    "| Count only non-null values | `df.select(count(\"salary\")).show()` |\n",
    "| Count NULL values explicitly | `df.select(count(when(col(\"salary\").isNull(), 1))).show()` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c07e9a-75d1-4074-be6c-93d8e73b06cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_csv.dropna(subset=['commission_pct']).display()      # show only not null rows in commission column.\n",
    "\n",
    "#df_csv.fillna(0).show()             # if it is number you have you fill with numeric\n",
    "\n",
    "df_csv.fillna(0,subset=['commission_pct']).show(5)      # Filling only in commission column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de763c1-3e8b-4e17-be57-89e3a5187f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Split, Indexing and Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133185c2-e68e-4b65-8d10-8a8fa5b03627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_csv.withColumn('job_id',split('job_id','_')).show()      # Spliting\n",
    "\n",
    "df_csv.withColumn('job_id',split('job_id','_')[1]).show()   # showing only index 1 means 2nd value \n",
    "\n",
    "\n",
    "df_csv.withColumn('job_id_array', split(col('job_id'), '_')) \\              # Explode into another row\n",
    "      .withColumn('job_id_exploded', explode(col('job_id_array'))) \\\n",
    "      .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971ae77f-f72f-426c-b489-f61cb61fd8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Œ GroupBY ()\n",
    "#  Aggregation Functions\n",
    "\n",
    "| Function | Example | Description |\n",
    "|----------|---------|-------------|\n",
    "| `count()` | `df.groupBy(\"dept\").count().show()` | Counts rows for each group |\n",
    "| `sum()` | `df.groupBy(\"dept\").sum(\"salary\").show()` | Sum of a numeric column |\n",
    "| `avg()` / `mean()` | `df.groupBy(\"dept\").avg(\"salary\").show()` | Average value |\n",
    "| `min()` | `df.groupBy(\"dept\").min(\"salary\").show()` | Minimum value |\n",
    "| `max()` | `df.groupBy(\"dept\").max(\"salary\").show()` | Maximum value |\n",
    "| `agg()` | `df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_salary\"))` | Multiple aggregations in one query |\n",
    "| `collect_list()` | `df.collect_list(\"first_name\").show()` | collect all names for each group |\n",
    "| `pivot()` | `df.groupBy(\"year\").pivot(\"dept\").sum(\"salary\")` | Pivot table for better visualization |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e1f950-993c-45ed-82f2-bdcb8511eeb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----------+----------+----------+--------------------+\n|department_id|count_emp|sum_salary|avg_salary|min_salary|max_salary|  collect_list_Names|\n+-------------+---------+----------+----------+----------+----------+--------------------+\n|         null|        1|      7000|    7000.0|      7000|      7000|           Kimberely|\n|           10|        1|      4400|    4400.0|      4400|      4400|            Jennifer|\n|           20|        2|     19000|    9500.0|      6000|     13000|        Michael, Pat|\n|           30|        6|     24900|    4150.0|      2500|     11000|Den, Alexander, S...|\n|           40|        1|      6500|    6500.0|      6500|      6500|               Susan|\n|           50|       45|    156400|    3476.0|      2100|      8200|Matthew, Adam, Pa...|\n|           60|        5|     28800|    5760.0|      4200|      9000|Alexander, Bruce,...|\n|           70|        1|     10000|   10000.0|     10000|     10000|             Hermann|\n|           80|       34|    304500|    8956.0|      6100|     14000|John, Karen, Albe...|\n|           90|        3|     59000|   19667.0|     17000|     25000|  Steven, Neena, Lex|\n|          100|        6|     51600|    8600.0|      6900|     12000|Nancy, Daniel, Jo...|\n|          110|        2|     20300|   10150.0|      8300|     12000|    Shelley, William|\n+-------------+---------+----------+----------+----------+----------+--------------------+\n\nroot\n |-- department_id: integer (nullable = true)\n |-- count_emp: long (nullable = false)\n |-- sum_salary: long (nullable = true)\n |-- avg_salary: double (nullable = true)\n |-- min_salary: integer (nullable = true)\n |-- max_salary: integer (nullable = true)\n |-- collect_list_Names: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_grp = df_csv.groupBy('department_id')\\\n",
    "                    .agg(\n",
    "                        count('employee_id').alias('count_emp'),        # count of the emp in each department\n",
    "                        sum('salary').alias('sum_salary'),              # sum of the emp in each department\n",
    "                        round(avg('salary')).alias('avg_salary'),\n",
    "                        min('salary').alias('min_salary'),\n",
    "                        max('salary').alias('max_salary'),\n",
    "                        concat_ws(\", \", collect_list('first_name')).alias('collect_list_Names')  # Convert list to string\n",
    "    )\n",
    "\n",
    "df_grp.sort(col('department_id').asc()).show()\n",
    "\n",
    "df_grp.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79609c72-caa6-47ab-ab55-36733bd41bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b72b50-3120-47a2-98c6-c8eeec76b898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.groupBy('job_id').pivot('department_id').sum('salary').show(5)       # Use pivot() when you need a pivot table format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd127cc-fd18-41ec-86a0-99d5180f87e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## When-Otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd0c7e5-752e-40a5-8a14-94f613a0f9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+------------+\n|employee_id|first_name|last_name|   email|phone_number| hire_date| job_id|salary|commission_pct|manager_id|department_id|salary_range|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+------------+\n|        100|    Steven|     King|   SKING|515.123.4567|2087-06-17|AD_PRES| 25000|          null|      null|           90|   PRESENENT|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2089-09-21|  AD_VP| 17000|          null|       100|           90|          VP|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2093-01-13|  AD_VP| 17000|          null|       100|           90|          VP|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|2090-01-03|IT_PROG|  9000|          null|       102|           60|   employees|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|2091-05-21|IT_PROG|  6000|          null|       103|           60|   employees|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "df_csv.withColumn('salary_range',when(col('salary')>20000,'PRESENENT')\\                             # This is like a CASE Statement in SQL.\n",
    "                                .when((col('salary')>=15000) & (col('salary')<=20000) ,'VP')\n",
    "                                .when((col('salary')>=10000) & (col('salary')<=15000) ,'Manager')\n",
    "                                .otherwise('employees')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b37d456-c0fc-4bc1-98de-8c4d92651b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d733de88-84d2-42c6-8138-f7594269f047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading emp and department table in data frame\n",
    "#from pyspark.sql.functions import *\n",
    "df_emp = spark.read.csv('dbfs:/FileStore/tables/employees.csv',inferSchema=True,header=True)\n",
    "df_emp.show(5)\n",
    "df_dept = spark.read.csv('dbfs:/FileStore/tables/departments.csv',inferSchema=True,header=True)\n",
    "df_dept.show(5)\n",
    "\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'inner').show()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'left').show()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'right').show()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'full').display()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b21d96-fab6-4e94-be44-e32dfa567ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Window Functions\n",
    "## ðŸ”¹Common Window Functions in PySpark\n",
    "\n",
    "| **Function** | **Description** | **Example Usage** |\n",
    "|-------------|---------------|------------------|\n",
    "| `row_number()` | Assigns a unique row number within a partition | `df.withColumn(\"row_num\", row_number().over(windowSpec))` |\n",
    "| `rank()` | Assigns a rank, but skips ranks for duplicate values | `df.withColumn(\"rank\", rank().over(windowSpec))` |\n",
    "| `dense_rank()` | Assigns a rank without skipping values | `df.withColumn(\"dense_rank\", dense_rank().over(windowSpec))` |\n",
    "| `lag(col, n)` | Gets the previous rowâ€™s value in a partition | `df.withColumn(\"prev_salary\", lag(\"salary\", 1).over(windowSpec))` |\n",
    "| `lead(col, n)` | Gets the next rowâ€™s value in a partition | `df.withColumn(\"next_salary\", lead(\"salary\", 1).over(windowSpec))` |\n",
    "| `sum(col)` | Running total (cumulative sum) | `df.withColumn(\"cumulative_salary\", sum(\"salary\").over(windowSpec))` |\n",
    "| `avg(col)` | Running average | `df.withColumn(\"avg_salary\", avg(\"salary\").over(windowSpec))` |\n",
    "| `max(col)` | Running maximum | `df.withColumn(\"max_salary\", max(\"salary\").over(windowSpec))` |\n",
    "| `min(col)` | Running minimum | `df.withColumn(\"min_salary\", min(\"salary\").over(windowSpec))` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5ce156-07fc-43a8-af44-cd268f0fc72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+\n|employee_id|first_name| hire_date|  job_id|salary|commission_pct|manager_id|department_id|row_num|rank|densc_rnk|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+\n|        178| Kimberely|2099-05-24|  SA_REP|  7000|          0.15|       149|         null|      1|   1|        1|\n|        200|  Jennifer|2087-09-17| AD_ASST|  4400|          null|       101|           10|      1|   1|        1|\n|        201|   Michael|2096-02-17|  MK_MAN| 13000|          null|       100|           20|      1|   1|        1|\n|        202|       Pat|2097-08-17|  MK_REP|  6000|          null|       201|           20|      2|   2|        2|\n|        114|       Den|2094-12-07|  PU_MAN| 11000|          null|       100|           30|      1|   1|        1|\n|        115| Alexander|2095-05-18|PU_CLERK|  3100|          null|       114|           30|      2|   2|        2|\n|        116|    Shelli|2097-12-24|PU_CLERK|  2900|          null|       114|           30|      3|   3|        3|\n|        117|     Sigal|2097-07-24|PU_CLERK|  2800|          null|       114|           30|      4|   4|        4|\n|        118|       Guy|2098-11-15|PU_CLERK|  2600|          null|       114|           30|      5|   5|        5|\n|        119|     Karen|2099-08-10|PU_CLERK|  2500|          null|       114|           30|      6|   6|        6|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "windowSpec = Window.partitionBy('department_id').orderBy(col(\"salary\").desc())  # you can remove partitionBy \n",
    "\n",
    "df_csv = df_csv.drop('phone_number','email','last_name')\n",
    "\n",
    "df_csv = df_csv.withColumn('row_num',row_number().over(windowSpec))\\\n",
    "               .withColumn('rank',rank().over(windowSpec))\\\n",
    "               .withColumn('densc_rnk',dense_rank().over(windowSpec))\n",
    "df_csv.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3de622-c15c-4b6e-bed0-aefae13efa8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cumulative Sum:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeedd782-0fa8-4950-854c-7b6cc084423a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------------------+----------+--------+------+--------------+----------+-------------+-----------------+\n|employee_id|first_name| last_name|   email|      phone_number| hire_date|  job_id|salary|commission_pct|manager_id|department_id|cumulative_salary|\n+-----------+----------+----------+--------+------------------+----------+--------+------+--------------+----------+-------------+-----------------+\n|        178| Kimberely|     Grant|  KGRANT|011.44.1644.429263|2099-05-24|  SA_REP|  7000|          0.15|       149|         null|             7000|\n|        200|  Jennifer|    Whalen| JWHALEN|      515.123.4444|2087-09-17| AD_ASST|  4400|          null|       101|           10|             4400|\n|        202|       Pat|       Fay|    PFAY|      603.123.6666|2097-08-17|  MK_REP|  6000|          null|       201|           20|             6000|\n|        201|   Michael| Hartstein|MHARTSTE|      515.123.5555|2096-02-17|  MK_MAN| 13000|          null|       100|           20|            19000|\n|        119|     Karen|Colmenares|KCOLMENA|      515.127.4566|2099-08-10|PU_CLERK|  2500|          null|       114|           30|             2500|\n|        118|       Guy|    Himuro| GHIMURO|      515.127.4565|2098-11-15|PU_CLERK|  2600|          null|       114|           30|             5100|\n|        117|     Sigal|    Tobias| STOBIAS|      515.127.4564|2097-07-24|PU_CLERK|  2800|          null|       114|           30|             7900|\n|        116|    Shelli|     Baida|  SBAIDA|      515.127.4563|2097-12-24|PU_CLERK|  2900|          null|       114|           30|            10800|\n|        115| Alexander|      Khoo|   AKHOO|      515.127.4562|2095-05-18|PU_CLERK|  3100|          null|       114|           30|            13900|\n|        114|       Den|  Raphaely|DRAPHEAL|      515.127.4561|2094-12-07|  PU_MAN| 11000|          null|       100|           30|            24900|\n+-----------+----------+----------+--------+------------------+----------+--------+------+--------------+----------+-------------+-----------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# commululative Sum:-\n",
    "#from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# Define Window for Cumulative Sum (Partition by Department, Ordered by Salary)\n",
    "windowSpecCumulative = Window.partitionBy(\"department_id\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_csv = df_csv.withColumn(\"cumulative_salary\", sum(\"salary\").over(windowSpecCumulative))\n",
    "df_csv.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a56a9a-3460-4174-b046-d14777cdacd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# User Defind Function:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f259eb-b404-4cad-ac33-7a59b43b982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+-----------------+-----------+\n|employee_id|first_name| hire_date|  job_id|salary|commission_pct|manager_id|department_id|row_num|rank|densc_rnk|cumulative_salary|full_salary|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+-----------------+-----------+\n|        178| Kimberely|2099-05-24|  SA_REP|  7000|          0.15|       149|         null|      1|   1|        1|             7000|    7000.15|\n|        200|  Jennifer|2087-09-17| AD_ASST|  4400|          null|       101|           10|      1|   1|        1|             4400|       null|\n|        202|       Pat|2097-08-17|  MK_REP|  6000|          null|       201|           20|      2|   2|        2|             6000|       null|\n|        201|   Michael|2096-02-17|  MK_MAN| 13000|          null|       100|           20|      1|   1|        1|            19000|       null|\n|        119|     Karen|2099-08-10|PU_CLERK|  2500|          null|       114|           30|      6|   6|        6|             2500|       null|\n|        118|       Guy|2098-11-15|PU_CLERK|  2600|          null|       114|           30|      5|   5|        5|             5100|       null|\n|        117|     Sigal|2097-07-24|PU_CLERK|  2800|          null|       114|           30|      4|   4|        4|             7900|       null|\n|        116|    Shelli|2097-12-24|PU_CLERK|  2900|          null|       114|           30|      3|   3|        3|            10800|       null|\n|        115| Alexander|2095-05-18|PU_CLERK|  3100|          null|       114|           30|      2|   2|        2|            13900|       null|\n|        114|       Den|2094-12-07|  PU_MAN| 11000|          null|       100|           30|      1|   1|        1|            24900|       null|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+-----------------+-----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, coalesce, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a Python function\n",
    "def calculate_full_salary(salary, commission):\n",
    "    if salary is None:\n",
    "        salary = 0\n",
    "    if commission is None:\n",
    "        commission = 0\n",
    "    return salary + commission\n",
    "\n",
    "# Register as a UDF\n",
    "calculate_full_salary_udf = udf(calculate_full_salary, DoubleType())\n",
    "\n",
    "# Use in DataFrame\n",
    "df_full_sal = df_csv.withColumn(\n",
    "    \"full_salary\", \n",
    "    calculate_full_salary_udf(col(\"salary\"), col(\"commission_pct\"))\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_full_sal.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a092e08f-39f2-4308-8dd3-e42917908ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“ DATA WRITING in PySpark\n",
    "\n",
    "PySpark provides multiple methods to write data to different formats and destinations.\n",
    "\n",
    "ðŸš€ Best Practices.\n",
    "\n",
    "âœ… Use Parquet/Delta for efficient storage.\n",
    "\n",
    "âœ… Use Partitioning for large datasets.\n",
    "\n",
    "âœ… Use Append Mode for incremental data updates.\n",
    "\n",
    "âœ… Use Overwrite Mode carefully to avoid accidental data loss.\n",
    "\n",
    "\n",
    "Why Parquet is Efficient for Storage?\n",
    "\n",
    "1ï¸âƒ£ Columnar Storage Format\n",
    "Unlike CSV (row-based format), Parquet stores data column-wise.\n",
    "This improves compression and query performance by scanning only required columns.\n",
    "\n",
    "ðŸ“Œ Example Query:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "SELECT salary FROM employees WHERE department_id = 10;\n",
    "In CSV, all columns are read before filtering.\n",
    "In Parquet, only salary & department_id columns are read, making it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa87a09-5df8-4cd9-9bf1-5508cf68ab99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 1. Writing DataFrame to CSV\n",
    "df_grp.write.format('csv').option('header',True).save('/FileStore/tables/Dept_Grp_csv')\n",
    "\n",
    "## 2. Writing DataFrame to JSON\n",
    "df.write.format(\"json\").save(\"/FileStore/tables/output_json\")\n",
    "\n",
    "\n",
    "## 3. Writing DataFrame to Parquet (Optimized Storage Format)\n",
    "df.write.format(\"parquet\").save(\"/FileStore/tables/output_parquet\")\n",
    "\n",
    "\n",
    "## 4. Writing DataFrame to Delta Format (For Databricks)\n",
    "df.write.format(\"delta\").save(\"/mnt/delta/output_delta\")\n",
    "\n",
    "## 5. Writing DataFrame to a Database (JDBC)\n",
    "df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/srk\") \\\n",
    "    .option(\"dbtable\", \"employees\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5dae1cd-bd74-4c59-b38a-d1b91e90454e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 6. Overwriting & Appending Data\n",
    "df.write.mode(\"overwrite\").csv(\"/FileStore/tables/output_csv\")\n",
    "\n",
    "## Append to Existing Data\n",
    "df.write.mode(\"append\").csv(\"/FileStore/tables/output_csv\")\n",
    "\n",
    "\n",
    "## 7. Writing Data in Partitioned Format\n",
    "df.write.partitionBy(\"department_id\").parquet(\"/FileStore/tables/output_partitioned\")\n",
    "\n",
    "## 8. Writing Data in Bucketing Format\n",
    "df.write.bucketBy(4, \"employee_id\").saveAsTable(\"bucketed_employees\")\n",
    "\n",
    "## Error\n",
    "df.write.format('csv').mode('error').option('path','/FileStore/tables/CSV/data.csv').save()\n",
    "\n",
    "## Ignore\n",
    "df.write.format('csv').mode('ignore').option('path','/FileStore/tables/CSV/data.csv').save()\n",
    "\n",
    "# Creating Table : -\n",
    "df.write.format('parquet').mode('overwrite').saveAsTable('my_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a3e86ad-8b32-4e35-b8f7-bc989183dd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1604b206-e564-4ea4-a4fb-2269441c899b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.createTempView('emp')\n",
    "\n",
    "%sql\n",
    "\n",
    "select * from emp where department_id = 60\n",
    "\n",
    "\n",
    "df_sql = spark.sql(\"select * from emp where department_id = 60\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_basics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

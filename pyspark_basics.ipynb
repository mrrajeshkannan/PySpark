{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fec91e1c-6f81-49dd-be61-9e1b4f020fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Basic To advanced PySpark query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c321960-8408-4d51-b34d-41f8ee2acaf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DATA READING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68951798-1121-4867-9834-02102c98d7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Data from CSV:\n",
    "df_csv = spark.read.csv('/FileStore/tables/employees.csv',inferSchema=True, header=True).display()\n",
    "display(df_csv.limit(5))        # Alter method\n",
    "df_csv.display()                # Alter method\n",
    "df_csv.show(5)  # Returns a list of Row objects\n",
    "\n",
    "\n",
    "\n",
    "# inferSchema means auto identify the data type in CSV / Json file.\n",
    "\n",
    "# same as json \n",
    "df_json = spark.read.json('/FileStore/tables/employees.json',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393f036e-59d2-4ec5-9780-41c872719065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Reading Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0df3031-2af5-4d33-9b3c-09d467255a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [FileInfo(path='dbfs:/FileStore/tables/departments.csv', name='departments.csv', size=687, modificationTime=1738674464000),\n FileInfo(path='dbfs:/FileStore/tables/emp.json', name='emp.json', size=752, modificationTime=1739564468000),\n FileInfo(path='dbfs:/FileStore/tables/employees.csv', name='employees.csv', size=7920, modificationTime=1738674451000)]"
     ]
    }
   ],
   "source": [
    "# Csv/Json uploaded locatations in Data bricks\n",
    "dbutils.fs.ls('dbfs:/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db4ec9c-ab42-4cbb-bd6a-7fe207b60163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Delete a File in locatation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3b349a-2b7f-4256-b826-f9553f335a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: True"
     ]
    }
   ],
   "source": [
    "#Delete a Single File\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/tables/employees-1.csv\")\n",
    "\n",
    "\n",
    "#Delete All Files in the Folder\n",
    "#dbutils.fs.rm(\"dbfs:/FileStore/tables/\", recurse=True)   #The recurse=True flag ensures all files in the folder are deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c5abb4-9aed-4074-9536-5f0181a98687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema** **Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429d35fb-25ce-4bfd-a469-c258e8933cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- hire_date: date (nullable = true)\n |-- job_id: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- commission_pct: double (nullable = true)\n |-- manager_id: integer (nullable = true)\n |-- department_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print the description of table / Scheme\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3ae35e-4308-4940-bc08-165af901f57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Select query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1025d3e9-0674-4cf3-a4e8-2872f6af35cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.display()   # Shows all \n",
    "\n",
    "df_csv.show(5)      # Alter method\n",
    "\n",
    "df_csv.select('first_name','salary','department_id').show(5)    # Selecting few columns we can't do column name change in this method\n",
    "\n",
    "df_csv.select(col('first_name').alias('Name'),col('salary'),col('department_id')).show(5)    # Selecting few columns with alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4a2a55-0b75-49a6-9116-83a136734b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Select with filter condition\n",
    "The filter transformation in PySpark allows you to select rows from a DataFrame based on a condition. \n",
    "\n",
    "### ðŸš€ Summary of `filter()` Use Cases  \n",
    "\n",
    "| Use Case               | Example  |\n",
    "|------------------------|----------|\n",
    "| **Logical Operators:** | `& (AND): (condition1) & (condition2) ` |\n",
    "| **Equality:**   | `col(\"column_name\") == value or col(\"column_name\") === value (The === is preferred for clarity)` |\n",
    "| **Inequality:** | `col(\"column_name\") != value or col(\"column_name\") <> value` |\n",
    "| **Greater than:** | `col(\"column_name\") > value` |\n",
    "| **Less than:** | `col(\"column_name\") < value` |\n",
    "| **Greater than or equal to:** | `col(\"column_name\") >= value` |\n",
    "| **Less than or equal to:** | `col(\"column_name\") <= value` |\n",
    "| **Handle NULL values** | `col(\"column_name\").isNull()` |\n",
    "| **Extract date parts** | `col(\"column_name\").isNotNull()` |\n",
    "| **Conditional logic** | `withColumn(\"salary_category\", when(col(\"salary\") > 60000, \"High\").otherwise(\"Low\"))` |\n",
    "| **Mathematical operations** | `withColumn(\"yearly_salary\", col(\"salary\") * 12)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c6bc2a-2e51-4f8c-9573-b59b25ed46fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.filter(col('department_id')==60).show(10)\n",
    "\n",
    "df_csv.filter(col('department_id').isin(60,80)).show(5)\n",
    "\n",
    "df_csv.filter(~col('department_id').isin(60,80)).show()    # selecting Notin \n",
    "\n",
    "df_csv.filter((col('department_id').isNotNull()) & (col('commission_pct').isNull())).show(5)\n",
    "\n",
    "\n",
    "\n",
    "df_csv.filter(\"department_id IS NOT NULL AND commission_pct IS NULL\").show(5)  # Alter SQL Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e68b1e-acde-4f2a-8eb8-b7aeff05dbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# withColumn\n",
    "The withColumn() function in PySpark allows you to add, update, or modify columns in a DataFrame.\n",
    "\n",
    "### ðŸš€ Summary of `withColumn()` Use Cases  \n",
    "\n",
    "| Use Case               | Example  |\n",
    "|------------------------|----------|\n",
    "| **Add a new column**   | `withColumn(\"bonus\", lit(500))` |\n",
    "| **Modify existing column** | `withColumn(\"salary\", col(\"salary\") * 1.1)` |\n",
    "| **Rename column** | `withColumn(\"emp_name\", col(\"first_name\")).drop(\"first_name\")` |\n",
    "| **Change data type** | `withColumn(\"department_id\", col(\"department_id\").cast(\"string\"))` |\n",
    "| **String transformations** | `withColumn(\"first_name_upper\", upper(col(\"first_name\")))` |\n",
    "| **Replace values** | `withColumn(\"job_id\", regexp_replace(col(\"job_id\"), \"FI_ACCOUNT\", \"ACNT\"))` |\n",
    "| **Handle NULL values** | `withColumn(\"commission_pct\", coalesce(col(\"commission_pct\"), lit(0)))` |\n",
    "| **Extract date parts** | `withColumn(\"hire_year\", year(col(\"hire_date\")))` |\n",
    "| **Conditional logic** | `withColumn(\"salary_category\", when(col(\"salary\") > 60000, \"High\").otherwise(\"Low\"))` |\n",
    "| **Mathematical operations** | `withColumn(\"yearly_salary\", col(\"salary\") * 12)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0dba79-1531-4fb9-9a60-e3490f0aed80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce, round\n",
    "\n",
    "\n",
    "df_csv.withColumn(\"bonus\", lit(500)).show(5)  # Adds a new column with a fixed value\n",
    "\n",
    "df_csv.withColumn(\n",
    "    \"Full_sal\", round((col(\"salary\")) * (1 + coalesce(col(\"commission_pct\"), lit(0))), 2) # Round to 2 decimal places\n",
    ").display()\n",
    "\n",
    "\n",
    "df_new = df_csv.withColumn(\"department_id\", col(\"department_id\").cast(\"string\"))\n",
    "df_new.printSchema()          # \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b62b307c-8772-491e-972e-4b37188ad3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## StructType() Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ed188f-426c-440f-984f-52983e3e67ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**importing** **libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9d94dc-1622-4c9a-b5ca-1e9db626ed1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a50b96c-b5eb-4c3e-9e0d-8e0d042624e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sorting ascending / descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02a320e-703b-4ac7-88de-43d93fe82de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#   Method 1\n",
    "df_csv.sort(col('department_id').desc()).show(5)\n",
    "\n",
    "df_csv.orderBy(col(\"salary\").desc()).show(5)  # Alternative\n",
    "\n",
    "df_csv.sort(col('department_id').asc()).show(5) # Ascending order\n",
    "\n",
    "\n",
    "#   Method 2 with 2 or more column \n",
    "df_csv.sort(['department_id','salary'],ascending = [1,0]).show(10)     # passing 0 is false 1 is True for assending order\n",
    "\n",
    "df_csv.sort(['first_name'],ascending = [1]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebc1c825-ea22-4d3e-8d65-2521c073dfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- hire_date: date (nullable = true)\n |-- job_id: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- commission_pct: double (nullable = true)\n |-- manager_id: integer (nullable = true)\n |-- department_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "n.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9abbab79-2275-4d40-ab54-99864f22e46a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "445ff3d7-a9ed-48b5-8e2d-0bad71b8ac36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_basics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fec91e1c-6f81-49dd-be61-9e1b4f020fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Basic To advanced PySpark query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15b36b8-cd2e-4558-b3b8-d01d67b8c318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Cluster vs. Distributed System\n",
    "\n",
    "| Feature               | **Cluster**                                      | **Distributed System**                             |\n",
    "|----------------------|--------------------------------|---------------------------------|\n",
    "| **Definition**       | A group of interconnected computers (nodes) that work together as a single unit. | A collection of independent computers that work together but appear as a single system to users. |\n",
    "| **Architecture**     | Usually tightly coupled with shared resources (e.g., shared storage). | Loosely coupled; nodes communicate over a network. |\n",
    "| **Data Sharing**     | Nodes often have access to shared storage. | Nodes manage data separately and communicate over a network. |\n",
    "| **Fault Tolerance**  | High availability; failure of one node doesn't affect others significantly. | More fault-tolerant; designed for redundancy and recovery. |\n",
    "| **Example Technologies** | Databricks, Hadoop Cluster, Kubernetes Cluster | Apache Spark, AWS, Google Cloud, Distributed Databases |\n",
    "\n",
    "## ✅ Key Differences\n",
    "1. **A cluster is a type of distributed system**, but not all distributed systems are clusters.\n",
    "2. **Clusters are typically within a single data center**, while distributed systems **can span across multiple locations**.\n",
    "3. **Clusters often have a shared resource model**, while distributed systems focus on **scalability and fault tolerance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c321960-8408-4d51-b34d-41f8ee2acaf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DATA READING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68951798-1121-4867-9834-02102c98d7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+\n|employee_id|first_name|last_name|   email|phone_number| hire_date| job_id|salary|commission_pct|manager_id|department_id|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+\n|        100|    Steven|     King|   SKING|515.123.4567|2087-06-17|AD_PRES| 25000|          null|      null|           90|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2089-09-21|  AD_VP| 17000|          null|       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2093-01-13|  AD_VP| 17000|          null|       100|           90|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|2090-01-03|IT_PROG|  9000|          null|       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|2091-05-21|IT_PROG|  6000|          null|       103|           60|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Read Data from CSV:\n",
    "df_csv = spark.read.csv('dbfs:/FileStore/tables/employees.csv',inferSchema=True, header=True)\n",
    "#display(df_csv.limit(5))        # Alter method\n",
    "#df_csv.display()                # Alter method\n",
    "df_csv.show(5)  # Returns a list of Row objects\n",
    "\n",
    "\n",
    "\n",
    "# inferSchema means auto identify the data type in CSV / Json file.\n",
    "\n",
    "# same as json \n",
    "#df_json = spark.read.json('/FileStore/tables/employees.json',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393f036e-59d2-4ec5-9780-41c872719065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Reading Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0df3031-2af5-4d33-9b3c-09d467255a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [FileInfo(path='dbfs:/FileStore/tables/departments-1.csv', name='departments-1.csv', size=687, modificationTime=1740000633000),\n FileInfo(path='dbfs:/FileStore/tables/departments.csv', name='departments.csv', size=687, modificationTime=1738674464000),\n FileInfo(path='dbfs:/FileStore/tables/emp.json', name='emp.json', size=752, modificationTime=1739564468000),\n FileInfo(path='dbfs:/FileStore/tables/employees.csv', name='employees.csv', size=7920, modificationTime=1738674451000)]"
     ]
    }
   ],
   "source": [
    "# Csv/Json uploaded locatations in Data bricks\n",
    "dbutils.fs.ls('dbfs:/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db4ec9c-ab42-4cbb-bd6a-7fe207b60163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Delete a File in locatation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3b349a-2b7f-4256-b826-f9553f335a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Delete a Single File\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/tables/employees-1.csv\")\n",
    "\n",
    "\n",
    "#Delete All Files in the Folder\n",
    "#dbutils.fs.rm(\"dbfs:/FileStore/tables/\", recurse=True)   #The recurse=True flag ensures all files in the folder are deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c5abb4-9aed-4074-9536-5f0181a98687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema** **Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429d35fb-25ce-4bfd-a469-c258e8933cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print the description of table / Scheme\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3ae35e-4308-4940-bc08-165af901f57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Select query \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1025d3e9-0674-4cf3-a4e8-2872f6af35cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.display()   # Shows all \n",
    "\n",
    "df_csv.show(5)      # Alter method\n",
    "\n",
    "df_csv.select('first_name','salary','department_id').show(5)    # Selecting few columns we can't do column name change in this method\n",
    "\n",
    "df_csv.select(col('first_name').alias('Name'),col('salary'),col('department_id')).show(5)    # Selecting few columns with alias\n",
    "\n",
    "df_csv = spark.read.csv('/FileStore/tables/employees.csv',inferSchema=True, header=True)\n",
    "\n",
    "df_csv.describe().display()     # data frame describtion like count, mean, median, max min sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4a2a55-0b75-49a6-9116-83a136734b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Select with filter condition\n",
    "The filter transformation in PySpark allows you to select rows from a DataFrame based on a condition. \n",
    "\n",
    "### 🚀 Summary of `filter()` Use Cases  \n",
    "\n",
    "| Use Case               | Example  |\n",
    "|------------------------|----------|\n",
    "| **Logical Operators:(and)** | `& (AND): (condition1) & (condition2)`|\n",
    "| **Equality:**   | `col(\"column_name\") == value or col(\"column_name\") === value (The === is preferred for clarity)` |\n",
    "| **Inequality:** | `col(\"column_name\") != value or col(\"column_name\") <> value` |\n",
    "| **Greater than:** | `col(\"column_name\") > value` |\n",
    "| **Less than:** | `col(\"column_name\") < value` |\n",
    "| **Greater than or equal to:** | `col(\"column_name\") >= value` |\n",
    "| **Less than or equal to:** | `col(\"column_name\") <= value` |\n",
    "| **Handle NULL values** | `col(\"column_name\").isNull()` |\n",
    "| **Extract date parts** | `col(\"column_name\").isNotNull()` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c6bc2a-2e51-4f8c-9573-b59b25ed46fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.filter(col('department_id')==60).show(10)\n",
    "\n",
    "df_csv.filter(col('department_id').isin(60,80)).show(5)\n",
    "\n",
    "df_csv.filter(~col('department_id').isin(60,80)).show()    # selecting Notin \n",
    "\n",
    "df_csv.filter((col('department_id').isNotNull()) & (col('commission_pct').isNull())).show(5)\n",
    "\n",
    "\n",
    "\n",
    "df_csv.filter(\"department_id IS NOT NULL AND commission_pct IS NULL\").show(5)  # Alter SQL Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e68b1e-acde-4f2a-8eb8-b7aeff05dbd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# withColumn\n",
    "The withColumn() function in PySpark allows you to add, update, or modify columns in a DataFrame.\n",
    "\n",
    "### 🚀 Summary of `withColumn()` Use Cases  \n",
    "\n",
    "| Use Case               | Example  |\n",
    "|------------------------|----------|\n",
    "| **Add a new column**   | `withColumn(\"bonus\", lit(500))` |\n",
    "| **Modify existing column** | `withColumn(\"salary\", col(\"salary\") * 1.1)` |\n",
    "| **Rename column** | `withColumn(\"emp_name\", col(\"first_name\")).drop(\"first_name\")` |\n",
    "| **Change data type** | `withColumn(\"department_id\", col(\"department_id\").cast(\"string\"))` |\n",
    "| **String transformations** | `withColumn(\"first_name_upper\", upper(col(\"first_name\")))` |\n",
    "| **Replace values** | `withColumn(\"job_id\", regexp_replace(col(\"job_id\"), \"FI_ACCOUNT\", \"ACNT\"))` |\n",
    "| **Handle NULL values** | `withColumn(\"commission_pct\", coalesce(col(\"commission_pct\"), lit(0)))` |\n",
    "| **Extract date parts** | `withColumn(\"hire_year\", year(col(\"hire_date\")))` |\n",
    "| **Conditional logic** | `withColumn(\"salary_category\", when(col(\"salary\") > 60000, \"High\").otherwise(\"Low\"))` |\n",
    "| **Mathematical operations** | `withColumn(\"yearly_salary\", col(\"salary\") * 12)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0dba79-1531-4fb9-9a60-e3490f0aed80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce, round\n",
    "\n",
    "# When you use withColumn the hole column and additional column will contain\n",
    "df_csv.withColumn(\"bonus\", lit(500)).show(5)  # Adds a new column with a fixed value\n",
    "\n",
    "df_csv.withColumn(\n",
    "    \"Full_sal\", round((col(\"salary\")) * (1 + coalesce(col(\"commission_pct\"), lit(0))), 2) # Round to 2 decimal places\n",
    ").display()\n",
    "\n",
    "\n",
    "df_new = df_csv.withColumn(\"department_id\", col(\"department_id\").cast(\"string\"))\n",
    "\n",
    "\n",
    "\n",
    "# When you use select only selected column will contain\n",
    "df_csv.select(col('first_name'), round((col(\"salary\")) * (1 + coalesce(col(\"commission_pct\"), lit(0)).alias(\"full_salary\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e0f1db-7a8f-403b-a4e6-a60424a01d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Changing the column type DDL :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ca4293-ba67-4eb4-87e0-36b92584e6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_csv = df_csv.withColumn(\"employee_id\", col(\"employee_id\").cast(\"string\"))\n",
    "\n",
    "from pyspark.sql.functions import to_date;\n",
    "df.withColumn('date', to_date(df['date_string']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b62b307c-8772-491e-972e-4b37188ad3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## StructType() Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ed188f-426c-440f-984f-52983e3e67ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**importing** **libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9d94dc-1622-4c9a-b5ca-1e9db626ed1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a50b96c-b5eb-4c3e-9e0d-8e0d042624e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sorting ascending / descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02a320e-703b-4ac7-88de-43d93fe82de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#   Method 1\n",
    "df_csv.sort(col('department_id').desc()).show(5)\n",
    "\n",
    "df_csv.orderBy(col(\"salary\").desc()).show(5)  # Alternative\n",
    "\n",
    "df_csv.sort(col('department_id').asc()).show(5) # Ascending order\n",
    "\n",
    "\n",
    "#   Method 2 with 2 or more column \n",
    "df_csv.sort(['department_id','salary'],ascending = [1,0]).show(10)     # passing 0 is false 1 is True for assending order\n",
    "\n",
    "df_csv.sort(['first_name'],ascending = [1]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1b99fe-e12b-4824-a9c2-611873bc79e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Type Casting:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc1c825-ea22-4d3e-8d65-2521c073dfae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_new = df_csv.withColumn('department_id',col('department_id').cast(StringType()))\n",
    "#df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b063bf2-2eba-4d54-927c-3c9d9d72f191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95dcef9-858a-4bfe-b082-0e6d9a08e901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop / remove column:-\n",
    "df_csv.drop('phone_number','email').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c752536-de0c-4074-9252-9571f6b82559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Drop_Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53125f2f-abbc-491e-a73a-69e51dec34f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove duplicates with all columns \n",
    "df_csv.distinct().display()\n",
    "\n",
    "\n",
    "# remove duplicates with all column and specfic column also \n",
    "df_csv.dropDuplicates().display()\n",
    "\n",
    "df_dis = df_csv.dropDuplicates(subset=['employee_id'])      # when you go with column duplicate use this method\n",
    "df_dis.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a03a67-53ee-4d28-b0a0-3672da37d763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Count number of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c5ac47-d63f-40f8-8fb5-b25b7f25f909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 1: Using count() action\n",
    "row_count = df_csv.count()\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "\n",
    "# Method 2: Using describe() to get summary statistics (includes count)\n",
    "summary = df_csv.describe()\n",
    "row_count_from_summary = int(summary.filter(\"summary == 'count'\").collect()[0][1]) #extracting the count value\n",
    "print(f\"Number of rows (from describe): {row_count_from_summary}\")\n",
    "\n",
    "# Method 3: Using length (if you've already collected the data - less efficient for large datasets)\n",
    "# Avoid this for large DataFrames as it brings all data to the driver\n",
    "# df_collected = df_csv.collect()  # Only use if DataFrame is small\n",
    "# row_count_collected = len(df_collected)\n",
    "# print(f\"Number of rows (collected): {row_count_collected}\")\n",
    "\n",
    "# Method 4: Using rdd.count()\n",
    "row_count_rdd = df_csv.rdd.count()\n",
    "print(f\"Number of rows (rdd): {row_count_rdd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9d22fb-caef-457a-b4c0-221a112785af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Union & UnionByName\n",
    "\n",
    "### 🔍 Key Differences in PySpark Union Functions\n",
    "\n",
    "| Method | Handles Column Order? | Handles Extra Columns? | Removes Duplicates? |\n",
    "|--------|------------------|------------------|------------------|\n",
    "| `union()` | ❌ No | ❌ No | ❌ No |\n",
    "| `unionAll()` | ❌ No | ❌ No | ❌ No |\n",
    "| `union().distinct()` | ❌ No | ❌ No | ✅ Yes |\n",
    "| `unionByName()` | ✅ Yes | ❌ No | ❌ No |\n",
    "| `unionByName(allowMissingColumns=True)` | ✅ Yes | ✅ Yes | ❌ No |\n",
    "\n",
    "---\n",
    "### **💡 Best Practices**\n",
    "- Use **`unionByName()`** when working with DataFrames that have **different column orders**.\n",
    "- Use **`allowMissingColumns=True`** when columns are missing between DataFrames.\n",
    "- Use **`.distinct()`** to **remove duplicates** after a union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8049c2dd-7170-480e-9003-278ce6322ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Preparatation : \n",
    "data1 = [('kad','1'),\n",
    "           ('sid','2')]\n",
    "schema1 = 'name STRING, id STRING' \n",
    "\n",
    "df1 = spark.createDataFrame(data1,schema1)        # creating data frame 1\n",
    "\n",
    "\n",
    "data2 = [('3','rahul'),\n",
    "        ('4','jas')]\n",
    "schema2 = 'id STRING, name STRING' \n",
    "\n",
    "df2 = spark.createDataFrame(data2,schema2)      # creating data frame 2\n",
    "\n",
    "#df1.display()   \n",
    "df1.display()\n",
    "df2.display()\n",
    "\n",
    "df1.union(df2).show()\n",
    "\n",
    "df1.unionByName(df2).show()         # See the difference in result \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48632257-ad9a-449c-a8bf-565d7abcbacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📌 String Functions\n",
    "\n",
    "PySpark provides various string functions to manipulate and process string data. Below is a categorized list of commonly used **string functions**:\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. Basic String Operations\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `upper()` | Converts to uppercase | `df.select(upper(col(\"name\"))).show()` |\n",
    "| `lower()` | Converts to lowercase | `df.select(lower(col(\"name\"))).show()` |\n",
    "| `length()` | Returns string length | `df.select(length(col(\"name\"))).show()` |\n",
    "| `trim()` | Removes spaces from both sides | `df.select(trim(col(\"name\"))).show()` |\n",
    "| `ltrim()` | Removes spaces from the left | `df.select(ltrim(col(\"name\"))).show()` |\n",
    "| `rtrim()` | Removes spaces from the right | `df.select(rtrim(col(\"name\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. Substring & Character Extraction\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `substring(col, start, length)` | Extracts substring from a column | `df.select(substring(col(\"name\"), 1, 3)).show()` |\n",
    "| `split(col, pattern)` | Splits string into an array | `df.select(split(col(\"name\"), \" \")).show()` |\n",
    "| `instr(col, substr)` | Finds position of substring | `df.select(instr(col(\"name\"), \"a\")).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. String Replacement & Formatting\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `regexp_replace(col, pattern, replacement)` | Replaces substrings using regex | `df.select(regexp_replace(col(\"name\"), \"John\", \"Mike\")).show()` |\n",
    "| `translate(col, fromStr, toStr)` | Replaces characters | `df.select(translate(col(\"name\"), \"abc\", \"xyz\")).show()` |\n",
    "| `format_string(format, col1, col2, ...)` | Formats string | `df.select(format_string(\"%s - %s\", col(\"name\"), col(\"job\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. String Case & Padding\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `initcap(col)` | Capitalizes first letter of each word | `df.select(initcap(col(\"name\"))).show()` |\n",
    "| `lpad(col, length, pad)` | Left-pads string with characters | `df.select(lpad(col(\"name\"), 10, \"0\")).show()` |\n",
    "| `rpad(col, length, pad)` | Right-pads string with characters | `df.select(rpad(col(\"name\"), 10, \"0\")).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 5. String Comparison & Matching\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `like(col, pattern)` | SQL-like pattern matching (`%`, `_`) | `df.filter(col(\"name\").like(\"A%\")).show()` |\n",
    "| `rlike(col, regex)` | Regex pattern matching | `df.filter(col(\"name\").rlike(\"^A.*\")).show()` |\n",
    "| `contains(col, substr)` | Checks if string contains substring | `df.filter(col(\"name\").contains(\"John\")).show()` |\n",
    "| `startsWith(col, prefix)` | Checks if string starts with substring | `df.filter(col(\"name\").startswith(\"A\")).show()` |\n",
    "| `endsWith(col, suffix)` | Checks if string ends with substring | `df.filter(col(\"name\").endswith(\"z\")).show()` |\n",
    "\n",
    "\n",
    "##  🔹 6. Concat two or more columns\n",
    "\n",
    "| Method | Use Case | Example |\n",
    "|--------|---------|---------|\n",
    "| `concat(col1, col2)` | Joins multiple string columns | `concat(col(\"first_name\"), col(\"last_name\"))` |\n",
    "| `concat_ws(\" \", col1, col2)` | Joins columns with a separator | `concat_ws(\"-\", col(\"city\"), col(\"state\"))` |\n",
    "| `concat(col1, lit(\" \"), col2)` | Adds static text between columns | `concat(col(\"first_name\"), lit(\" - \"), col(\"last_name\"))` |\n",
    "| `concat(col1, col2.cast(\"string\"))` | Converts non-string columns before joining | `concat(col(\"first_name\"), col(\"salary\").cast(\"string\"))` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e22375-b0b6-4442-af91-7a1733562b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, trim, ltrim, rtrim, substring, split, instr, regexp_replace, translate, format_string, initcap, lpad, rpad\n",
    "\n",
    "from pyspark.sql.functions import *     # you can use this \n",
    "\n",
    "df_csv.select(col('first_name'), upper(col(\"first_name\")), \n",
    "              lower(col(\"first_name\")).alias('Name'), length(col(\"first_name\")),\n",
    "              concat(col('first_name'), col('last_name')).alias(\"full_name\") ,\n",
    "              concat(col('first_name'), lit(' '), col('last_name')).alias(\"full_name\") \n",
    "            ).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c49872f5-b03a-436b-a007-5e71b8930f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📆 Date Functions  \n",
    "\n",
    "PySpark provides various **date and time functions** for handling and manipulating date-related data. Below is a categorized list of commonly used **date functions** in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **1. Extracting Date Components**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `year(col(\"date\"))` | Extracts the year from a date | `df.select(year(col(\"hire_date\"))).show()` |\n",
    "| `month(col(\"date\"))` | Extracts the month from a date | `df.select(month(col(\"hire_date\"))).show()` |\n",
    "| `day(col(\"date\"))` | Extracts the day of the month | `df.select(dayofmonth(col(\"hire_date\"))).show()` |\n",
    "| `dayofweek(col(\"date\"))` | Extracts the day of the week (1 = Sunday, 7 = Saturday) | `df.select(dayofweek(col(\"hire_date\"))).show()` |\n",
    "| `dayofyear(col(\"date\"))` | Extracts the day of the year | `df.select(dayofyear(col(\"hire_date\"))).show()` |\n",
    "| `quarter(col(\"date\"))` | Extracts the quarter of the year | `df.select(quarter(col(\"hire_date\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **2. Date Arithmetic & Manipulation**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `date_add(col(\"date\"), n)` | Adds `n` days to a date | `df.select(date_add(col(\"hire_date\"), 10)).show()` |\n",
    "| `date_sub(col(\"date\"), n)` | Subtracts `n` days from a date | `df.select(date_sub(col(\"hire_date\"), 10)).show()` |\n",
    "| `add_months(col(\"date\"), n)` | Adds `n` months to a date | `df.select(add_months(col(\"hire_date\"), 3)).show()` |\n",
    "| `months_between(col(\"date1\"), col(\"date2\"))` | Returns months between two dates | `df.select(months_between(col(\"hire_date\"), col(\"current_date\"))).show()` |\n",
    "| `next_day(col(\"date\"), \"Day\")` | Returns the next occurrence of the specified day | `df.select(next_day(col(\"hire_date\"), \"Friday\")).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **3. Converting & Formatting Dates**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `to_date(col(\"string_col\"), \"format\")` | Converts string to date | `df.select(to_date(col(\"date_str\"), \"yyyy-MM-dd\")).show()` |\n",
    "| `date_format(col(\"date\"), \"format\")` | Formats a date column | `df.select(date_format(col(\"hire_date\"), \"yyyy/MM/dd\")).show()` |\n",
    "| `unix_timestamp(col(\"date\"))` | Converts date to Unix timestamp | `df.select(unix_timestamp(col(\"hire_date\"))).show()` |\n",
    "| `from_unixtime(col(\"unix_col\"))` | Converts Unix timestamp to date | `df.select(from_unixtime(col(\"unix_col\"))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **4. Time Functions**\n",
    "| Function | Description | Example |\n",
    "|----------|------------|---------|\n",
    "| `current_date()` | Returns the current system date | `df.select(current_date()).show()` |\n",
    "| `current_timestamp()` | Returns the current timestamp | `df.select(current_timestamp()).show()` |\n",
    "| `datediff(col(\"date1\"), col(\"date2\"))` | Returns days between two dates | `df.select(datediff(col(\"hire_date\"), current_date())).show()` |\n",
    "| `timestamp_seconds(col(\"unix_col\"))` | Converts Unix timestamp (seconds) to timestamp | `df.select(timestamp_seconds(col(\"unix_col\"))).show()` |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ca5085-f620-43ad-aeee-631fae52cb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col, year, month, dayofmonth, date_add, to_date, current_date, datediff\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "df_csv.select(\n",
    "                col('hire_date'),\n",
    "                year(col('hire_date')).alias('year'),\n",
    "                month(col('hire_date')).alias('Mon'),\n",
    "                dayofmonth(col('hire_date')).alias('day'),\n",
    "                dayofweek('hire_date').alias('week_of_mon'),\n",
    "                add_months(col('hire_date'),3).alias('add_mon'),\n",
    "                add_months(col('hire_date'),-3).alias('sub_mon'),\n",
    "                date_add(col('hire_date'),5).alias('add_date'),\n",
    "                date_sub(col('hire_date'),5).alias('add_date'),\n",
    "                date_trunc('Month',col('hire_date')).alias('first_day_of_mon'),\n",
    "                datediff(current_date(),col('hire_date')).alias('date_diff'),\n",
    "                current_date()\n",
    "            ).show(5)\n",
    "\n",
    "\n",
    "# here showing difference between withColumn and select \n",
    "df_csv.withColumn('current_Date', current_date()).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9569b96c-fc16-4bb2-9ad5-3a652c38ee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🚀 Handling NULLs \n",
    "\n",
    "Handling **NULL** values is crucial when working with large datasets in PySpark. Below is a summary of common techniques to manage **NULL** values effectively.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **1. Filtering NULL Values**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Filter rows where column is NOT NULL | `df.filter(col(\"salary\").isNotNull()).show()` |\n",
    "| Filter rows where column IS NULL | `df.filter(col(\"salary\").isNull()).show()` |\n",
    "| Drop rows containing NULLs in any column | `df.na.drop().show()` |\n",
    "| Drop rows only if specific column(s) are NULL | `df.na.drop(subset=[\"salary\"]).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **2. Replacing NULL Values**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Replace NULLs with a default value | `df.na.fill(0).show()` |\n",
    "| Replace NULLs in specific column | `df.na.fill({\"salary\": 50000, \"department\": \"Unknown\"}).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **3. Using `coalesce()` for Default Values**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Replace NULLs with another column value | `df.withColumn(\"final_salary\", coalesce(col(\"salary\"), lit(50000))).show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **4. Handling NULLs in Aggregations**\n",
    "| Use Case | Example |\n",
    "|----------|---------|\n",
    "| Ignore NULLs in aggregations (default behavior) | `df.select(avg(\"salary\")).show()` |\n",
    "| Count only non-null values | `df.select(count(\"salary\")).show()` |\n",
    "| Count NULL values explicitly | `df.select(count(when(col(\"salary\").isNull(), 1))).show()` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c07e9a-75d1-4074-be6c-93d8e73b06cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_csv.dropna(subset=['commission_pct']).display()      # show only not null rows in commission column.\n",
    "\n",
    "#df_csv.fillna(0).show()             # if it is number you have you fill with numeric\n",
    "\n",
    "df_csv.fillna(0,subset=['commission_pct']).show(5)      # Filling only in commission column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de763c1-3e8b-4e17-be57-89e3a5187f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Split, Indexing and Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133185c2-e68e-4b65-8d10-8a8fa5b03627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_csv.withColumn('job_id',split('job_id','_')).show()      # Spliting\n",
    "\n",
    "df_csv.withColumn('job_id',split('job_id','_')[1]).show()   # showing only index 1 means 2nd value \n",
    "\n",
    "\n",
    "df_csv.withColumn('job_id_array', split(col('job_id'), '_')) \\              # Explode into another row\n",
    "      .withColumn('job_id_exploded', explode(col('job_id_array'))) \\\n",
    "      .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971ae77f-f72f-426c-b489-f61cb61fd8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📌 GroupBY ()\n",
    "#  Aggregation Functions\n",
    "\n",
    "| Function | Example | Description |\n",
    "|----------|---------|-------------|\n",
    "| `count()` | `df.groupBy(\"dept\").count().show()` | Counts rows for each group |\n",
    "| `sum()` | `df.groupBy(\"dept\").sum(\"salary\").show()` | Sum of a numeric column |\n",
    "| `avg()` / `mean()` | `df.groupBy(\"dept\").avg(\"salary\").show()` | Average value |\n",
    "| `min()` | `df.groupBy(\"dept\").min(\"salary\").show()` | Minimum value |\n",
    "| `max()` | `df.groupBy(\"dept\").max(\"salary\").show()` | Maximum value |\n",
    "| `agg()` | `df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_salary\"))` | Multiple aggregations in one query |\n",
    "| `collect_list()` | `df.collect_list(\"first_name\").show()` | collect all names for each group |\n",
    "| `pivot()` | `df.groupBy(\"year\").pivot(\"dept\").sum(\"salary\")` | Pivot table for better visualization |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e1f950-993c-45ed-82f2-bdcb8511eeb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----------+----------+----------+--------------------+\n|department_id|count_emp|sum_salary|avg_salary|min_salary|max_salary|  collect_list_Names|\n+-------------+---------+----------+----------+----------+----------+--------------------+\n|         null|        1|      7000|    7000.0|      7000|      7000|           Kimberely|\n|           10|        1|      4400|    4400.0|      4400|      4400|            Jennifer|\n|           20|        2|     19000|    9500.0|      6000|     13000|        Michael, Pat|\n|           30|        6|     24900|    4150.0|      2500|     11000|Den, Alexander, S...|\n|           40|        1|      6500|    6500.0|      6500|      6500|               Susan|\n|           50|       45|    156400|    3476.0|      2100|      8200|Matthew, Adam, Pa...|\n|           60|        5|     28800|    5760.0|      4200|      9000|Alexander, Bruce,...|\n|           70|        1|     10000|   10000.0|     10000|     10000|             Hermann|\n|           80|       34|    304500|    8956.0|      6100|     14000|John, Karen, Albe...|\n|           90|        3|     59000|   19667.0|     17000|     25000|  Steven, Neena, Lex|\n|          100|        6|     51600|    8600.0|      6900|     12000|Nancy, Daniel, Jo...|\n|          110|        2|     20300|   10150.0|      8300|     12000|    Shelley, William|\n+-------------+---------+----------+----------+----------+----------+--------------------+\n\nroot\n |-- department_id: integer (nullable = true)\n |-- count_emp: long (nullable = false)\n |-- sum_salary: long (nullable = true)\n |-- avg_salary: double (nullable = true)\n |-- min_salary: integer (nullable = true)\n |-- max_salary: integer (nullable = true)\n |-- collect_list_Names: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_grp = df_csv.groupBy('department_id')\\\n",
    "                    .agg(\n",
    "                        count('employee_id').alias('count_emp'),        # count of the emp in each department\n",
    "                        sum('salary').alias('sum_salary'),              # sum of the emp in each department\n",
    "                        round(avg('salary')).alias('avg_salary'),\n",
    "                        min('salary').alias('min_salary'),\n",
    "                        max('salary').alias('max_salary'),\n",
    "                        concat_ws(\", \", collect_list('first_name')).alias('collect_list_Names')  # Convert list to string\n",
    "    )\n",
    "\n",
    "df_grp.sort(col('department_id').asc()).show()\n",
    "\n",
    "df_grp.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79609c72-caa6-47ab-ab55-36733bd41bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b72b50-3120-47a2-98c6-c8eeec76b898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.groupBy('job_id').pivot('department_id').sum('salary').show(5)       # Use pivot() when you need a pivot table format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd127cc-fd18-41ec-86a0-99d5180f87e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## When-Otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd0c7e5-752e-40a5-8a14-94f613a0f9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+------------+\n|employee_id|first_name|last_name|   email|phone_number| hire_date| job_id|salary|commission_pct|manager_id|department_id|salary_range|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+------------+\n|        100|    Steven|     King|   SKING|515.123.4567|2087-06-17|AD_PRES| 25000|          null|      null|           90|   PRESENENT|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|2089-09-21|  AD_VP| 17000|          null|       100|           90|          VP|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|2093-01-13|  AD_VP| 17000|          null|       100|           90|          VP|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|2090-01-03|IT_PROG|  9000|          null|       102|           60|   employees|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|2091-05-21|IT_PROG|  6000|          null|       103|           60|   employees|\n+-----------+----------+---------+--------+------------+----------+-------+------+--------------+----------+-------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "df_csv.withColumn('salary_range',when(col('salary')>20000,'PRESENENT')\\                             # This is like a CASE Statement in SQL.\n",
    "                                .when((col('salary')>=15000) & (col('salary')<=20000) ,'VP')\n",
    "                                .when((col('salary')>=10000) & (col('salary')<=15000) ,'Manager')\n",
    "                                .otherwise('employees')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b37d456-c0fc-4bc1-98de-8c4d92651b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d733de88-84d2-42c6-8138-f7594269f047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading emp and department table in data frame\n",
    "#from pyspark.sql.functions import *\n",
    "df_emp = spark.read.csv('dbfs:/FileStore/tables/employees.csv',inferSchema=True,header=True)\n",
    "df_emp.show(5)\n",
    "df_dept = spark.read.csv('dbfs:/FileStore/tables/departments.csv',inferSchema=True,header=True)\n",
    "df_dept.show(5)\n",
    "\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'inner').show()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'left').show()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'right').show()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'full').display()\n",
    "\n",
    "df_in_join = df_emp.join(df_dept, df_emp['department_id']==df_dept['department_id'],'anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b21d96-fab6-4e94-be44-e32dfa567ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Window Functions\n",
    "## 🔹Common Window Functions in PySpark\n",
    "\n",
    "| **Function** | **Description** | **Example Usage** |\n",
    "|-------------|---------------|------------------|\n",
    "| `row_number()` | Assigns a unique row number within a partition | `df.withColumn(\"row_num\", row_number().over(windowSpec))` |\n",
    "| `rank()` | Assigns a rank, but skips ranks for duplicate values | `df.withColumn(\"rank\", rank().over(windowSpec))` |\n",
    "| `dense_rank()` | Assigns a rank without skipping values | `df.withColumn(\"dense_rank\", dense_rank().over(windowSpec))` |\n",
    "| `lag(col, n)` | Gets the previous row’s value in a partition | `df.withColumn(\"prev_salary\", lag(\"salary\", 1).over(windowSpec))` |\n",
    "| `lead(col, n)` | Gets the next row’s value in a partition | `df.withColumn(\"next_salary\", lead(\"salary\", 1).over(windowSpec))` |\n",
    "| `sum(col)` | Running total (cumulative sum) | `df.withColumn(\"cumulative_salary\", sum(\"salary\").over(windowSpec))` |\n",
    "| `avg(col)` | Running average | `df.withColumn(\"avg_salary\", avg(\"salary\").over(windowSpec))` |\n",
    "| `max(col)` | Running maximum | `df.withColumn(\"max_salary\", max(\"salary\").over(windowSpec))` |\n",
    "| `min(col)` | Running minimum | `df.withColumn(\"min_salary\", min(\"salary\").over(windowSpec))` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5ce156-07fc-43a8-af44-cd268f0fc72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+\n|employee_id|first_name| hire_date|  job_id|salary|commission_pct|manager_id|department_id|row_num|rank|densc_rnk|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+\n|        178| Kimberely|2099-05-24|  SA_REP|  7000|          0.15|       149|         null|      1|   1|        1|\n|        200|  Jennifer|2087-09-17| AD_ASST|  4400|          null|       101|           10|      1|   1|        1|\n|        201|   Michael|2096-02-17|  MK_MAN| 13000|          null|       100|           20|      1|   1|        1|\n|        202|       Pat|2097-08-17|  MK_REP|  6000|          null|       201|           20|      2|   2|        2|\n|        114|       Den|2094-12-07|  PU_MAN| 11000|          null|       100|           30|      1|   1|        1|\n|        115| Alexander|2095-05-18|PU_CLERK|  3100|          null|       114|           30|      2|   2|        2|\n|        116|    Shelli|2097-12-24|PU_CLERK|  2900|          null|       114|           30|      3|   3|        3|\n|        117|     Sigal|2097-07-24|PU_CLERK|  2800|          null|       114|           30|      4|   4|        4|\n|        118|       Guy|2098-11-15|PU_CLERK|  2600|          null|       114|           30|      5|   5|        5|\n|        119|     Karen|2099-08-10|PU_CLERK|  2500|          null|       114|           30|      6|   6|        6|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "windowSpec = Window.partitionBy('department_id').orderBy(col(\"salary\").desc())  # you can remove partitionBy \n",
    "\n",
    "df_csv = df_csv.drop('phone_number','email','last_name')\n",
    "\n",
    "df_csv = df_csv.withColumn('row_num',row_number().over(windowSpec))\\\n",
    "               .withColumn('rank',rank().over(windowSpec))\\\n",
    "               .withColumn('densc_rnk',dense_rank().over(windowSpec))\n",
    "df_csv.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3de622-c15c-4b6e-bed0-aefae13efa8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cumulative Sum:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeedd782-0fa8-4950-854c-7b6cc084423a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------------------+----------+--------+------+--------------+----------+-------------+-----------------+\n|employee_id|first_name| last_name|   email|      phone_number| hire_date|  job_id|salary|commission_pct|manager_id|department_id|cumulative_salary|\n+-----------+----------+----------+--------+------------------+----------+--------+------+--------------+----------+-------------+-----------------+\n|        178| Kimberely|     Grant|  KGRANT|011.44.1644.429263|2099-05-24|  SA_REP|  7000|          0.15|       149|         null|             7000|\n|        200|  Jennifer|    Whalen| JWHALEN|      515.123.4444|2087-09-17| AD_ASST|  4400|          null|       101|           10|             4400|\n|        202|       Pat|       Fay|    PFAY|      603.123.6666|2097-08-17|  MK_REP|  6000|          null|       201|           20|             6000|\n|        201|   Michael| Hartstein|MHARTSTE|      515.123.5555|2096-02-17|  MK_MAN| 13000|          null|       100|           20|            19000|\n|        119|     Karen|Colmenares|KCOLMENA|      515.127.4566|2099-08-10|PU_CLERK|  2500|          null|       114|           30|             2500|\n|        118|       Guy|    Himuro| GHIMURO|      515.127.4565|2098-11-15|PU_CLERK|  2600|          null|       114|           30|             5100|\n|        117|     Sigal|    Tobias| STOBIAS|      515.127.4564|2097-07-24|PU_CLERK|  2800|          null|       114|           30|             7900|\n|        116|    Shelli|     Baida|  SBAIDA|      515.127.4563|2097-12-24|PU_CLERK|  2900|          null|       114|           30|            10800|\n|        115| Alexander|      Khoo|   AKHOO|      515.127.4562|2095-05-18|PU_CLERK|  3100|          null|       114|           30|            13900|\n|        114|       Den|  Raphaely|DRAPHEAL|      515.127.4561|2094-12-07|  PU_MAN| 11000|          null|       100|           30|            24900|\n+-----------+----------+----------+--------+------------------+----------+--------+------+--------------+----------+-------------+-----------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# commululative Sum:-\n",
    "#from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "# Define Window for Cumulative Sum (Partition by Department, Ordered by Salary)\n",
    "windowSpecCumulative = Window.partitionBy(\"department_id\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_csv = df_csv.withColumn(\"cumulative_salary\", sum(\"salary\").over(windowSpecCumulative))\n",
    "df_csv.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a56a9a-3460-4174-b046-d14777cdacd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# User Defind Function:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f259eb-b404-4cad-ac33-7a59b43b982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+-----------------+-----------+\n|employee_id|first_name| hire_date|  job_id|salary|commission_pct|manager_id|department_id|row_num|rank|densc_rnk|cumulative_salary|full_salary|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+-----------------+-----------+\n|        178| Kimberely|2099-05-24|  SA_REP|  7000|          0.15|       149|         null|      1|   1|        1|             7000|    7000.15|\n|        200|  Jennifer|2087-09-17| AD_ASST|  4400|          null|       101|           10|      1|   1|        1|             4400|       null|\n|        202|       Pat|2097-08-17|  MK_REP|  6000|          null|       201|           20|      2|   2|        2|             6000|       null|\n|        201|   Michael|2096-02-17|  MK_MAN| 13000|          null|       100|           20|      1|   1|        1|            19000|       null|\n|        119|     Karen|2099-08-10|PU_CLERK|  2500|          null|       114|           30|      6|   6|        6|             2500|       null|\n|        118|       Guy|2098-11-15|PU_CLERK|  2600|          null|       114|           30|      5|   5|        5|             5100|       null|\n|        117|     Sigal|2097-07-24|PU_CLERK|  2800|          null|       114|           30|      4|   4|        4|             7900|       null|\n|        116|    Shelli|2097-12-24|PU_CLERK|  2900|          null|       114|           30|      3|   3|        3|            10800|       null|\n|        115| Alexander|2095-05-18|PU_CLERK|  3100|          null|       114|           30|      2|   2|        2|            13900|       null|\n|        114|       Den|2094-12-07|  PU_MAN| 11000|          null|       100|           30|      1|   1|        1|            24900|       null|\n+-----------+----------+----------+--------+------+--------------+----------+-------------+-------+----+---------+-----------------+-----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, coalesce, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a Python function\n",
    "def calculate_full_salary(salary, commission):\n",
    "    if salary is None:\n",
    "        salary = 0\n",
    "    if commission is None:\n",
    "        commission = 0\n",
    "    return salary + commission\n",
    "\n",
    "# Register as a UDF\n",
    "calculate_full_salary_udf = udf(calculate_full_salary, DoubleType())\n",
    "\n",
    "# Use in DataFrame\n",
    "df_full_sal = df_csv.withColumn(\n",
    "    \"full_salary\", \n",
    "    calculate_full_salary_udf(col(\"salary\"), col(\"commission_pct\"))\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_full_sal.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a092e08f-39f2-4308-8dd3-e42917908ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📝 DATA WRITING in PySpark\n",
    "\n",
    "PySpark provides multiple methods to write data to different formats and destinations.\n",
    "\n",
    "🚀 Best Practices.\n",
    "\n",
    "✅ Use Parquet/Delta for efficient storage.\n",
    "\n",
    "✅ Use Partitioning for large datasets.\n",
    "\n",
    "✅ Use Append Mode for incremental data updates.\n",
    "\n",
    "✅ Use Overwrite Mode carefully to avoid accidental data loss.\n",
    "\n",
    "\n",
    "Why Parquet is Efficient for Storage?\n",
    "\n",
    "1️⃣ Columnar Storage Format\n",
    "Unlike CSV (row-based format), Parquet stores data column-wise.\n",
    "This improves compression and query performance by scanning only required columns.\n",
    "\n",
    "📌 Example Query:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "SELECT salary FROM employees WHERE department_id = 10;\n",
    "In CSV, all columns are read before filtering.\n",
    "In Parquet, only salary & department_id columns are read, making it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa87a09-5df8-4cd9-9bf1-5508cf68ab99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 1. Writing DataFrame to CSV\n",
    "df_grp.write.format('csv').option('header',True).save('/FileStore/tables/Dept_Grp_csv')\n",
    "\n",
    "## 2. Writing DataFrame to JSON\n",
    "df.write.format(\"json\").save(\"/FileStore/tables/output_json\")\n",
    "\n",
    "\n",
    "## 3. Writing DataFrame to Parquet (Optimized Storage Format)\n",
    "df.write.format(\"parquet\").save(\"/FileStore/tables/output_parquet\")\n",
    "\n",
    "\n",
    "## 4. Writing DataFrame to Delta Format (For Databricks)\n",
    "df.write.format(\"delta\").save(\"/mnt/delta/output_delta\")\n",
    "\n",
    "## 5. Writing DataFrame to a Database (JDBC)\n",
    "df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/srk\") \\\n",
    "    .option(\"dbtable\", \"employees\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5dae1cd-bd74-4c59-b38a-d1b91e90454e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 6. Overwriting & Appending Data\n",
    "df.write.mode(\"overwrite\").csv(\"/FileStore/tables/output_csv\")\n",
    "\n",
    "## Append to Existing Data\n",
    "df.write.mode(\"append\").csv(\"/FileStore/tables/output_csv\")\n",
    "\n",
    "\n",
    "## 7. Writing Data in Partitioned Format\n",
    "df.write.partitionBy(\"department_id\").parquet(\"/FileStore/tables/output_partitioned\")\n",
    "\n",
    "## 8. Writing Data in Bucketing Format\n",
    "df.write.bucketBy(4, \"employee_id\").saveAsTable(\"bucketed_employees\")\n",
    "\n",
    "## Error\n",
    "df.write.format('csv').mode('error').option('path','/FileStore/tables/CSV/data.csv').save()\n",
    "\n",
    "## Ignore\n",
    "df.write.format('csv').mode('ignore').option('path','/FileStore/tables/CSV/data.csv').save()\n",
    "\n",
    "# Creating Table : -\n",
    "df.write.format('parquet').mode('overwrite').saveAsTable('my_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a3e86ad-8b32-4e35-b8f7-bc989183dd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1604b206-e564-4ea4-a4fb-2269441c899b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.createTempView('emp')\n",
    "\n",
    "%sql\n",
    "\n",
    "select * from emp where department_id = 60\n",
    "\n",
    "\n",
    "df_sql = spark.sql(\"select * from emp where department_id = 60\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_basics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
